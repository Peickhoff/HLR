1) 
	1. Jobs werden in eine Queue eingeführt. Elemente der Queue werden ausgeführt wenn ausreichen Ressourcen verfügbar sind. 
	2. Zuteilung von Ressourcen und Verteilung auf Knoten
	3. Slurm,Moab,Openlava
	4. Slurm
	5. Mit sbatch lassen sich Jobs in die Queue befördern. Der Befehl bekommt eine Datei mit den Auszuführenden Skripten, sowie Paramtern, wie Knotenanzahl, Dauer und anzahl Aufgaben, etc...
	6. Mit smap, sview, sinfo, scontorl show nodes oder squeue lassen sich Informationen über die Jobs anzeigen
	7. sview bietet die Möglichkeit über ein graphisches Interface Jobs anzuzeigen und zu bearbeiten
	8. Mit scancel lassen sich Jobs beenden.
	9. Es können keine Zwei Jobs auf einem Knoten laufen
	10. eickhoff@cluster:~$ scontrol -d show job 110455
		JobId=110455 JobName=bash
   		UserId=tietz1(1402) GroupId=papo-16(1037)
   		Priority=14 Nice=0 Account=(null) QOS=(null)
   		JobState=RUNNING Reason=None Dependency=(null)
   		Requeue=1 Restarts=0 BatchFlag=0 Reboot=0 ExitCode=0:0
   		DerivedExitCode=0:0
   		RunTime=01:31:08 TimeLimit=06:00:00 TimeMin=N/A
   		SubmitTime=2016-11-04T21:22:53 EligibleTime=2016-11-04T21:22:53
   		StartTime=2016-11-04T21:22:53 EndTime=2016-11-05T03:22:53
   		PreemptTime=None SuspendTime=None SecsPreSuspend=0
   		Partition=west AllocNode:Sid=cluster:3557
  		ReqNodeList=(null) ExcNodeList=(null)
   		NodeList=west1
   		BatchHost=west1
   		NumNodes=1 NumCPUs=24 CPUs/Task=1 ReqB:S:C:T=0:0:*:*
   		TRES=cpu=24,node=1
   		Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*
     		Nodes=west1 CPU_IDs=0-23 Mem=0
   		MinCPUsNode=1 MinMemoryNode=0 MinTmpDiskNode=0
   		Features=(null) Gres=(null) Reservation=(null)
   		Shared=0 Contiguous=0 Licenses=(null) Network=(null)
   		Command=(null)
   		WorkDir=/home/tietz1/HLR/03-PDE/pde
   		Power= SICP=0
	11. backfilling wird benutzt: 
	eickhoff@cluster:~$ cat /etc/slurm-llnl/slurm.conf 
# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=cluster
#ControlAddr=
#BackupController=
#BackupAddr=
#
AuthType=auth/munge
CacheGroups=0
#CheckpointType=checkpoint/none
CryptoType=crypto/munge
#DisableRootJobs=NO
#EnforcePartLimits=NO
#Epilog=/home/slurm/epilog.sh
#PrologSlurmctld=
#FirstJobId=1
JobCheckpointDir=/var/lib/slurm-llnl/checkpoint
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0
#JobRequeue=1
#KillOnBadExit=0
#Licenses=foo*4,bar
#MailProg=/bin/mail
#MaxJobCount=5000
#MaxTasksPerNode=128
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
#ProctrackType=proctrack/pgid
ProctrackType=proctrack/linuxproc
#Prolog=/home/slurm/prolog.sh
#PrologSlurmctld=
#PropagatePrioProcess=0
PropagateResourceLimits=NONE
#PropagateResourceLimitsExcept=
ReturnToService=2
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/tmp/slurm-llnl/slurmd
SlurmUser=slurm
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/tmp/slurm-llnl/slurmctld
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/affinity
TaskPluginParam=Sched
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFs=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UnkillableStepTimeout=
UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=300
SlurmdTimeout=300
#UnkillableStepProgram=
#UnkillableStepTimeout=60
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=0
#EnablePreemption=no
FastSchedule=1
#MaxMemPerCPU=0
#SchedulerRootFilter=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/linear
#SelectTypeParameters=
#
#
# JOB PRIORITY
# https://computing.llnl.gov/linux/slurm/priority_multifactor.html#config
PriorityType=priority/multifactor

# 2 week half-life
PriorityDecayHalfLife=14-0

# The larger the job, the greater its job size priority.
PriorityFavorSmall=NO

# The job's age factor reaches 1.0 after waiting in the
# queue for 2 weeks.
PriorityMaxAge=14-0

# This next group determines the weighting of each of the
# components of the Multi-factor Job Priority Plugin.
# The default value for each of the following is 1.
PriorityWeightAge=1000
PriorityWeightFairshare=100000
PriorityWeightJobSize=1000
PriorityWeightPartition=1000
PriorityWeightQOS=10000


#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
AccountingStorageHost=cluster.cluster
AccountingStoragePass=imSlURMadmin
#AccountingStoragePort=
AccountingStorageType=accounting_storage/filetxt
AccountingStorageLoc=/var/run/slurm-llnl/accounting
AccountingStorageUser=slurm

ClusterName=cluster
#DebugFlags=
JobCompHost=cluster.cluster
JobCompLoc=slurm_jobcomp_db
#JobCompLoc=/var/log/slurm-llnl/accounting-complete.txt
JobCompPass=imSlURMadmin
#JobCompPort=
JobCompType=jobcomp/mysql
JobCompUser=slurm
JobAcctGatherFrequency=10
JobAcctGatherType=jobacct_gather/linux
#AcctGatherProfileType=acct_gather_profile/hdf5 
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=/home/slurm/suspend.sh
#ResumeProgram=/home/slurm/resume.sh
#SuspendTimeout=60
#ResumeTimeout=300
#ResumeRate=10
#SuspendExcNodes=
#SuspendExcParts=abu,amd,magny,nehalem,sandy,west
#SuspendRate=10
#SuspendTime=900
#
#
# COMPUTE NODES
NodeName=abu[1-5] Sockets=4 CoresPerSocket=12 ThreadsPerCore=1 Feature=BigData State=UNKNOWN
NodeName=amd[1-5] Sockets=2 CoresPerSocket=12 ThreadsPerCore=1 State=UNKNOWN
NodeName=magny1 RealMemory=110000 Sockets=4 CoresPerSocket=12 ThreadsPerCore=1 State=UNKNOWN
NodeName=nehalem[1-5] Sockets=2 CoresPerSocket=4 ThreadsPerCore=1 State=UNKNOWN
NodeName=sandy[1,2,3,4-10] RealMemory=15000 Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 State=UNKNOWN
NodeName=west[1-10] RealMemory=11000 Sockets=2 CoresPerSocket=6 ThreadsPerCore=2 State=UNKNOWN

# All existing nodes
PartitionName=DEFAULT Shared=EXCLUSIVE MaxTime=06:00:00 State=UP

PartitionName=abu         Nodes=abu[1-5]
PartitionName=amd         Nodes=amd[1-5]
PartitionName=magny       Nodes=magny1 MaxTime=24:00:00
PartitionName=nehalem     Nodes=nehalem[1-5]
PartitionName=sandy       Nodes=sandy[1-10] AllowGroups=admin,io
PartitionName=west        Nodes=west[1-10]
PartitionName=reservation Nodes=west[1-10] Priority=10000 MaxTime=1-00:00:00 AllowGroups=wr,theses

	12.  eickhoff@cluster:~$ salloc -p west -N 1 -w west7
salloc: Granted job allocation 110456
eickhoff@cluster:~$ srun hostname
west7
eickhoff@cluster:~$ exit
exit
salloc: Relinquishing job allocation 110456
salloc: Job allocation 110456 has been revoked.

	13. 6 stunden bei alen Partitionen ausser magny 1 tag
	14. scontrol -d show job <jobid> zum zeigen der Priorität und scontrol top <jobid>, scontrol update Priority=number oder  scontrol uhold <jobid>
	15. abu, amd, magny, nehalem und west sind Partitionen die durch -p <Partiton> oder mit scontrol update Partition=  angegeben werden können.

2)
	4. Nichts auffälliges, Skirpt tut was es soll. Man könnte die timescript.out Datei auch im skript erstellen mit 'echo "..." >> timescript.out'. Das Problem ist nur, dass die Datei nicht überschrieben wird, sondern immer weiter appended.
